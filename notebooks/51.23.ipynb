{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18eef7f6-a4fa-4e16-afad-61ae83b9a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "import einops\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional, Union\n",
    "import xarray as xr\n",
    "from concurrent import futures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "from itertools import chain\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca5c8a6-f87b-4cd4-b029-cc2abc4c85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nowcasting_dataloader.data_sources.satellite.satellite_model import SAT_MEAN, SAT_STD\n",
    "from nowcasting_dataloader.data_sources.nwp.nwp_model import NWP_MEAN, NWP_STD\n",
    "\n",
    "# 1. **********\n",
    "from nowcasting_utils.metrics.validation import make_validation_results\n",
    "# **********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c9baa2-d9f6-4d9b-ab65-4b670d5bf12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_validation_results_to_logger(\n",
    "    results_dfs: list[pd.DataFrame],\n",
    "    results_file_name: str,\n",
    "    current_epoch: Union[int, str],\n",
    "    logger: Optional[NeptuneLogger] = None,\n",
    "):\n",
    "    # join all validation step results together\n",
    "    results_df = pd.concat(results_dfs)\n",
    "    results_df.reset_index(inplace=True)\n",
    "\n",
    "    # save to csv file\n",
    "    name_csv = f\"{results_file_name}_{current_epoch}.csv\"\n",
    "    results_df.to_csv(name_csv)\n",
    "\n",
    "    # upload csv to neptune\n",
    "    logger.experiment[f\"validation/results/epoch_{current_epoch}\"].upload(name_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e3b2dc-b415-455a-9104-20026a21a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pv_system_id_index_lut():\n",
    "    # In get_gsp_id_and_pv_system_index_for_each_pv_system(), we really don't want to have to loop round each example and each PV system.  \n",
    "    # So, instead, we create a \"look up table\" (LUT) where the entry at the i^th index\n",
    "    # is the index of the PV system (and len(passiv_2_minutely_pv_system_ids) elsewhere)\n",
    "    # (i.e. len(passiv_2_minutely_pv_system_ids) is used as a marker for \"NaN\"... it has to be a real int\n",
    "    # otherwise the Embedding blows up).\n",
    "    # That is, `lut[pv_system_id]` is the `index+1` of the PV system.\n",
    "    passiv_2_minutely_pv_system_ids = pd.read_csv(\"passiv_2_minutely_pv_system_IDs.csv\", index_col=\"index\", squeeze=True).to_list()\n",
    "    missing_pv_maker = len(passiv_2_minutely_pv_system_ids)+1\n",
    "    pv_system_id_index_lut = np.full(\n",
    "        shape=max(passiv_2_minutely_pv_system_ids)+1, \n",
    "        fill_value=missing_pv_maker, \n",
    "        dtype=np.int16\n",
    "    )\n",
    "    pv_system_id_index_lut[passiv_2_minutely_pv_system_ids] = np.arange(0, len(passiv_2_minutely_pv_system_ids), dtype=np.int16)\n",
    "    \n",
    "    # test\n",
    "    assert pv_system_id_index_lut[0] == missing_pv_maker  # No PV system pv_system_id 0\n",
    "    for i in [0, 1, 100, 1000, len(passiv_2_minutely_pv_system_ids)-1]:\n",
    "        pv_system_id = passiv_2_minutely_pv_system_ids[i]\n",
    "        assert pv_system_id_index_lut[pv_system_id] == i\n",
    "    return pv_system_id_index_lut\n",
    "\n",
    "\n",
    "pv_system_id_index_lut = get_pv_system_id_index_lut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb16937-c045-4657-ab19-a35f030c68e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a mapping from satellite pixel x and y coords to GSP ID\n",
    "gsp_id_per_satellite_pixel = xr.load_dataset(\"GSP_ID_per_satellite_pixel.nc\")[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32babc29-a1c8-4c6b-b3b8-5979efa5bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the number MAX_GSP_ID to indicate a \"missing GSP\" to the GSP ID embedding\n",
    "MAX_GSP_ID = int(np.nanmax(gsp_id_per_satellite_pixel.values) + 1)\n",
    "gsp_id_per_satellite_pixel = gsp_id_per_satellite_pixel.fillna(MAX_GSP_ID).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f11b36-0e6e-4b85-9630-e5bad0c438c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pv_system_id_to_gsp_lut():\n",
    "    \"\"\"Get a look up table (LUT) which maps from PV system ID to GSP ID.\n",
    "    \"\"\"\n",
    "    passiv_pv_system_id_to_gsp_id = pd.read_csv(\"passiv_pv_system_id_to_gsp_id.csv\", index_col=\"system_id\", squeeze=True)\n",
    "    missing_gsp_marker = MAX_GSP_ID,  # Use max(gsp_id)+1 as the marker for \"missing\" GSP\n",
    "    pv_system_id_to_gsp_lut = np.full(\n",
    "        shape=max(passiv_pv_system_id_to_gsp_id.index)+1, \n",
    "        fill_value=missing_gsp_marker,\n",
    "        dtype=np.int16\n",
    "    )\n",
    "    pv_system_id_to_gsp_lut[passiv_pv_system_id_to_gsp_id.index] = passiv_pv_system_id_to_gsp_id\n",
    "    \n",
    "    # test\n",
    "    assert pv_system_id_to_gsp_lut[0] == missing_gsp_marker\n",
    "    for i in [0, 1, 100, 1000, len(passiv_pv_system_id_to_gsp_id)-1]:\n",
    "        pv_system_id = passiv_pv_system_id_to_gsp_id.index[i]\n",
    "        gsp_id = passiv_pv_system_id_to_gsp_id.loc[pv_system_id]\n",
    "        assert pv_system_id_to_gsp_lut[pv_system_id] == gsp_id\n",
    "\n",
    "    return pv_system_id_to_gsp_lut\n",
    "\n",
    "\n",
    "pv_system_id_to_gsp_lut = get_pv_system_id_to_gsp_lut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf9bf72-3293-4a87-8814-814addd42962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gsp_id_and_pv_system_index_for_each_pv_system(pv_system_id_tensor: torch.Tensor) -> tuple[np.ndarray, np.array]:\n",
    "    \"\"\"For each PV system ID in batch['pv_system_id'], get the GSP ID and the \"index\" of the PV system (used for embedding the PV system ID).\"\"\"\n",
    "   \n",
    "    pv_system_id_tensor = torch.nan_to_num(\n",
    "        pv_system_id_tensor, \n",
    "        nan=0  # No PV systems have the ID \"0\".  So 0 is safe to use as a marker for \"missing\".\n",
    "    ).to(torch.int16)\n",
    "\n",
    "    pv_system_id_index = pv_system_id_index_lut[pv_system_id_tensor]\n",
    "    gsp_id_for_each_pv_system = pv_system_id_to_gsp_lut[pv_system_id_tensor]\n",
    "    return gsp_id_for_each_pv_system, pv_system_id_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6fade99-4a66-4d68-87c7-49eb2e409959",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"/mnt/storage_ssd_4tb/data/ocf/solar_pv_nowcasting/nowcasting_dataset_pipeline/prepared_ML_training_data/v15\")\n",
    "\n",
    "SATELLITE_CHANNELS = (\n",
    "    \"IR_016\",\n",
    "    \"IR_039\",\n",
    "    \"IR_087\",\n",
    "    \"IR_097\",\n",
    "    \"IR_108\",\n",
    "    \"IR_120\",\n",
    "    \"IR_134\",\n",
    "    \"VIS006\",\n",
    "    \"VIS008\",\n",
    "    \"WV_062\",\n",
    "    \"WV_073\",\n",
    ")\n",
    "\n",
    "NWP_CHANNELS = (\n",
    "    \"t\",\n",
    "    \"dswrf\",\n",
    "    \"prate\",\n",
    "    \"r\",\n",
    "    \"sde\",\n",
    "    \"si10\",\n",
    "    \"vis\",\n",
    "    \"lcc\",\n",
    "    \"mcc\",\n",
    "    \"hcc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97423b8f-dced-4159-be58-d4c37bf0fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def satellite_normalisation_stats_to_data_array(stat: dict, channel_names: tuple[str]) -> xr.DataArray:\n",
    "    return xr.DataArray(\n",
    "        [stat[chan_name] for chan_name in channel_names],\n",
    "        dims=\"channels_index\",\n",
    "    ).astype(np.float32).assign_coords(channels=(\"channels_index\", list(channel_names)))\n",
    "\n",
    "SAT_MEAN = satellite_normalisation_stats_to_data_array(SAT_MEAN, SATELLITE_CHANNELS)\n",
    "SAT_STD = satellite_normalisation_stats_to_data_array(SAT_STD, SATELLITE_CHANNELS)\n",
    "NWP_MEAN = satellite_normalisation_stats_to_data_array(NWP_MEAN, NWP_CHANNELS)\n",
    "NWP_STD = satellite_normalisation_stats_to_data_array(NWP_STD, NWP_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d7e8118-d999-489f-b915-f17ee75625e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_time(src: xr.Dataset, dst: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Align `dst` to have the same `time` coords as `src.time`.\n",
    "    \n",
    "    For example, use this to ensure that batch['opticalflow'] has the same\n",
    "    time coords as batch['gsp'].\n",
    "    \"\"\"\n",
    "    # I tried a bunch of \"vectorised\" ways of doing this.  This appears to be the\n",
    "    # only way of doing it.  The issue is that each example ends up having different\n",
    "    # time_index coords so, in order to align, we must reset the \"time_index\" of each example.\n",
    "    # We take the satellite image _15 minutes_ before the GSP data, because the GSP data is\n",
    "    # the average power for the half-hour-ending.\n",
    "    time_index = src.time - pd.Timedelta(\"15 minutes\")\n",
    "    n_timesteps = src.time.shape[1]\n",
    "    time_index_bool_mask = dst.time.isin(time_index)\n",
    "    data_arrays_for_examples = []\n",
    "    n_examples = len(time_index_bool_mask)\n",
    "    for example_i in range(n_examples):\n",
    "        selection = dst.isel(example=example_i, time_index=time_index_bool_mask[example_i])\n",
    "        # Ensure there are never too many timesteps\n",
    "        selection = selection.isel(time_index=slice(None, n_timesteps))\n",
    "        selection.__setitem__(\"time_index\", np.arange(len(selection[\"time_index\"])))\n",
    "        data_arrays_for_examples.append(selection)\n",
    "        \n",
    "    new_dst = xr.concat(data_arrays_for_examples, dim=\"example\")\n",
    "    return new_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cc598df-fdb0-4fed-b210-9d303ee1eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DATETIME_FEATURES = 4  # sin & cos for time of day and day of year\n",
    "\n",
    "\n",
    "def create_datetime_features(t0_datetimes: np.ndarray) -> dict[str, np.ndarray]:\n",
    "    t0_datetimes = pd.DatetimeIndex(t0_datetimes)\n",
    "    n_examples = len(t0_datetimes)\n",
    "\n",
    "    datetime_features = np.full(shape=(n_examples, N_DATETIME_FEATURES), fill_value=np.NaN, dtype=np.float32)\n",
    "\n",
    "    hour_of_day = t0_datetimes.hour + (t0_datetimes.minute / 60)\n",
    "    day_of_year = t0_datetimes.day_of_year + (hour_of_day / 24)\n",
    "\n",
    "    hour_of_day_radians = (hour_of_day / 24.0) * 2 * np.pi\n",
    "    day_of_year_radians = (day_of_year / 366 ) * 2 * np.pi  #  366 for leap years!\n",
    "\n",
    "    datetime_features[:, 0] = np.sin(hour_of_day_radians)\n",
    "    datetime_features[:, 1] = np.cos(hour_of_day_radians)\n",
    "    datetime_features[:, 2] = np.sin(day_of_year_radians)\n",
    "    datetime_features[:, 3] = np.cos(day_of_year_radians)\n",
    "    \n",
    "    return {\n",
    "        \"t0_datetime_features\": datetime_features,\n",
    "        \"t0_hour_of_day\": hour_of_day.values,\n",
    "        \"t0_day_of_year\": day_of_year.values,\n",
    "        \"t0_month\": t0_datetimes.month.values,\n",
    "        \"t0_datetime_UNIX_epoch\": t0_datetimes.values.astype(\"datetime64[s]\").astype(np.int32),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c13fabe-a589-4b9e-9e7d-0105d45918c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_x_and_y_and_time_coords(data_from_all_sources: dict[str, object], data_source_name: str):\n",
    "    data = data_from_all_sources[data_source_name]\n",
    "    data_from_all_sources[f\"{data_source_name}_time\"] = torch.from_numpy(data[\"time\"].values.astype(\"datetime64[s]\").astype(np.int32))\n",
    "    \n",
    "    if data_source_name in [\"satellite\", \"opticalflow\"]:\n",
    "        # satellite and opticalflow mistakenly include x and y coords for every timestep (even though they don't change across time!)\n",
    "        x_coords = data[\"x\"].isel(time_index=0)\n",
    "        y_coords = data[\"y\"].isel(time_index=0)\n",
    "    elif data_source_name in [\"gsp\", \"pv\"]:\n",
    "        # The x and y coords DataArrays are called \"x_coords\" and \"y_coords\" in gsp and pv.  And \"x\" and \"y\" in the others.\n",
    "        x_coords = data[\"x_coords\"]\n",
    "        y_coords = data[\"y_coords\"]\n",
    "        if data_source_name == \"pv\":\n",
    "            # Set the x_coords, y_coords and time for missing PV systems to NaN\n",
    "            mask_of_missing_pv_systems = np.isnan(data[\"data\"].values).any(axis=1)\n",
    "            x_coords.values[mask_of_missing_pv_systems] = np.NaN\n",
    "            y_coords.values[mask_of_missing_pv_systems] = np.NaN\n",
    "    else:\n",
    "        x_coords = data[\"x\"]\n",
    "        y_coords = data[\"y\"]\n",
    "        \n",
    "    data_from_all_sources[f\"{data_source_name}_x_coords\"] = torch.from_numpy(x_coords.values.astype(np.float32))\n",
    "    data_from_all_sources[f\"{data_source_name}_y_coords\"] = torch.from_numpy(y_coords.values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64fdc69e-ab1f-467c-942e-38465bf045d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimpleNowcastingDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        data_path: Base path to the pre-prepared dataset.  e.g. .../v15/train/\n",
    "        data_source_names: The names of the data sources.  Must also be the names of the subdirectory.  \n",
    "            Must include \"gsp\".\n",
    "        gsp_first_time_index_of_future: The index into the GSP time_index dimension that marks the start of the \"future\".\n",
    "        n_batches: The number of available batches.\n",
    "    \"\"\"\n",
    "    data_path: Path\n",
    "    data_source_names: Iterable[str]\n",
    "    gsp_first_time_index_of_future: int = 2\n",
    "    satellite_channels_index: Iterable[int] = np.arange(len(SATELLITE_CHANNELS))\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Sanity checks\n",
    "        assert self.data_path.exists()\n",
    "        assert len(self.data_source_names) > 0\n",
    "        assert \"gsp\" in self.data_source_names\n",
    "        self.n_batches = self._get_number_of_batches()\n",
    "        \n",
    "        self.sat_mean = SAT_MEAN.sel(channels_index=self.satellite_channels_index).drop(\"channels\")\n",
    "        self.sat_std = SAT_STD.sel(channels_index=self.satellite_channels_index).drop(\"channels\")\n",
    "        \n",
    "    def _get_number_of_batches(self) -> int:\n",
    "        \"\"\"Get number of batches.  Check every data source.\"\"\"\n",
    "        n_batches = None\n",
    "        for data_source_name in self.data_source_names:\n",
    "            path_for_data_source = self.data_path / data_source_name\n",
    "            n_batches_for_data_source = len(list(path_for_data_source.glob(\"*.nc\")))\n",
    "            if n_batches is None:\n",
    "                n_batches = n_batches_for_data_source\n",
    "            else:\n",
    "                assert n_batches == n_batches_for_data_source\n",
    "        assert n_batches is not None\n",
    "        assert n_batches > 0\n",
    "        return n_batches\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.n_batches\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Returned shapes:\n",
    "            gsp: batch_size, n_timesteps\n",
    "            opticalflow: \"example\", \"time_index\", \"channels_index\",  \"y_index\", \"x_index\"\n",
    "            nwp: \"example\", \"time_index\", \"channels_index\", \"y_index\", \"x_index\"\n",
    "            pv:\n",
    "        \"\"\"\n",
    "        data_from_all_sources = {}\n",
    "        # Parallelising this with concurrent.futures.ThreadPoolExecutor actually\n",
    "        # appears to be _slower_ than the simple loop approach!\n",
    "        for data_source_name in self.data_source_names:\n",
    "            filename = self.data_path / data_source_name / f\"{idx:06d}.nc\"\n",
    "            dataset = xr.open_dataset(filename)\n",
    "            \n",
    "            # Select just the \"future\" timesteps at half-hour intervals (t1, t2, etc.)\n",
    "            # for the first GSP (the \"target\").\n",
    "            if data_source_name == \"gsp\":\n",
    "                dataset = dataset.isel(\n",
    "                    time_index=slice(self.gsp_first_time_index_of_future, None),\n",
    "                    id_index=0\n",
    "                ).load()\n",
    "                # Normalise GSP\n",
    "                dataset[\"data\"] = dataset[\"power_mw\"] / dataset[\"capacity_mwp\"]\n",
    "                dataset[\"data\"] = dataset[\"data\"].astype(np.float32)\n",
    "                \n",
    "                # Datetime features\n",
    "                t0_datetimes = dataset.time.values[:, 0] - pd.Timedelta(\"30 minutes\")\n",
    "                \n",
    "            if data_source_name == \"pv\":\n",
    "                try:\n",
    "                    dataset = dataset.isel(time_index=slice(None, 7))\n",
    "                except ValueError as e:\n",
    "                    print(f\"Exception raised when reading PV: {filename}\", flush=True)\n",
    "                    raise\n",
    "                dataset[\"data\"] = dataset[\"power_mw\"] / dataset[\"capacity_mwp\"]\n",
    "            \n",
    "            if data_source_name in [\"satellite\", \"opticalflow\"]:\n",
    "                # Normalise satellite and opticalflow\n",
    "                dataset[\"data\"] = dataset[\"data\"].astype(np.float32)\n",
    "                dataset = dataset.sel(channels_index=self.satellite_channels_index)\n",
    "                dataset[\"data\"] -= self.sat_mean\n",
    "                dataset[\"data\"] /= self.sat_std\n",
    "                dataset[\"data\"] = dataset[\"data\"].transpose(\"example\", \"time_index\", \"channels_index\", \"y_index\", \"x_index\")\n",
    "                \n",
    "            if data_source_name == \"nwp\":\n",
    "                # Normalise satellite and opticalflow\n",
    "                dataset[\"data\"] -= NWP_MEAN.drop(\"channels\")\n",
    "                dataset[\"data\"] /= NWP_STD.drop(\"channels\")\n",
    "                dataset[\"data\"] = dataset[\"data\"].transpose(\"example\", \"time_index\", \"channels_index\", \"y_index\", \"x_index\")\n",
    "\n",
    "            data_from_all_sources[data_source_name] = dataset\n",
    "\n",
    "        if (\"satellite\" in self.data_source_names) and (\"opticalflow\" in self.data_source_names):\n",
    "            # Concatenate half an hour of satellite data to the start of the opticalflow data\n",
    "            # so we can use imagery 15 minutes before the GSP timestep, because the GSP data\n",
    "            # is half-hour-ending.\n",
    "            data_from_all_sources[\"opticalflow\"] = xr.concat(\n",
    "                (\n",
    "                    data_from_all_sources[\"satellite\"].isel(time_index=slice(None, 7)),\n",
    "                    data_from_all_sources[\"opticalflow\"]\n",
    "                ),\n",
    "                dim=\"time_index\",\n",
    "            )\n",
    "            del data_from_all_sources[\"satellite\"]\n",
    "\n",
    "        # Conform Satellite data sources to have the same time index as GSP\n",
    "        for data_source_name in [\"satellite\", \"opticalflow\", \"hrvsatellite\", \"hrvopticalflow\"]:\n",
    "            if data_source_name in data_from_all_sources:\n",
    "                data_from_all_sources[data_source_name] = align_time(\n",
    "                    src=data_from_all_sources[\"gsp\"],\n",
    "                    dst=data_from_all_sources[data_source_name].load())\n",
    "                \n",
    "        # Select just the data.  Grab other useful DataArrays.\n",
    "        new_data_source_names = list(data_from_all_sources.keys())\n",
    "        for data_source_name in new_data_source_names:\n",
    "            try:\n",
    "                copy_x_and_y_and_time_coords(data_from_all_sources, data_source_name)\n",
    "            except:\n",
    "                print(\"Exception raised while processing\", data_source_name)\n",
    "                raise\n",
    "\n",
    "            data = data_from_all_sources[data_source_name]\n",
    "\n",
    "            if data_source_name == \"pv\":\n",
    "                data_from_all_sources[\"pv_system_id\"] = torch.from_numpy(data[\"id\"].values.astype(np.float32))\n",
    "                pv_gsp_id, pv_system_id_index = get_gsp_id_and_pv_system_index_for_each_pv_system(data_from_all_sources[\"pv_system_id\"])\n",
    "                data_from_all_sources[\"pv_gsp_id\"] = torch.from_numpy(pv_gsp_id)\n",
    "                data_from_all_sources[\"pv_system_id_index\"] = torch.from_numpy(pv_system_id_index)\n",
    "                \n",
    "            if data_source_name == \"gsp\":\n",
    "                data_from_all_sources[\"gsp_id\"] = torch.from_numpy(data[\"id\"].values.astype(np.float32))\n",
    "                data_from_all_sources[\"capacity_mwp\"] = torch.from_numpy(data[\"capacity_mwp\"].values.astype(np.float32))\n",
    "\n",
    "            data_from_all_sources[data_source_name] = torch.from_numpy(data[\"data\"].values)\n",
    "            \n",
    "        # Datetime features.\n",
    "        t0_datetime_features = create_datetime_features(t0_datetimes)\n",
    "        for feature_name, feature_values in t0_datetime_features.items():\n",
    "            data_from_all_sources[feature_name] = torch.from_numpy(feature_values)\n",
    "        \n",
    "        # Get GSP ID for each satellite pixel\n",
    "        gsp_id_per_satellite_pixel_list = []\n",
    "        for x_coords, y_coords in zip(data_from_all_sources[\"opticalflow_x_coords\"], data_from_all_sources[\"opticalflow_y_coords\"]):      \n",
    "            selected_data = (\n",
    "                gsp_id_per_satellite_pixel.sel(\n",
    "                    x=x_coords,\n",
    "                    y=y_coords,\n",
    "                    method=\"nearest\",\n",
    "                )\n",
    "            )\n",
    "            gsp_id_per_satellite_pixel_list.append(selected_data)\n",
    "        data_from_all_sources[\"gsp_id_per_satellite_pixel\"] = torch.from_numpy(\n",
    "            np.stack(gsp_id_per_satellite_pixel_list)\n",
    "        )\n",
    "        \n",
    "        # Check for NaNs\n",
    "        for data_source_name, data in data_from_all_sources.items():\n",
    "            if data_source_name != \"gsp_id\" and not data_source_name.startswith(\"pv\") and np.isnan(data).any():\n",
    "                raise RuntimeError(f\"NaNs in {data_source_name} batch {idx}\")\n",
    "\n",
    "        return data_from_all_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf0eaa9-d678-499f-baa5-34ff474c7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE_NAMES = (\"gsp\", \"satellite\", \"pv\", \"opticalflow\", \"nwp\")\n",
    "SELECTED_SATELLITE_CHANNELS = [7, 8]  # both VIS channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56fe03c5-1dac-4d17-996c-3f716d7dc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SimpleNowcastingDataset(\n",
    "    data_path=DATA_PATH / \"train\",\n",
    "    data_source_names=DATA_SOURCE_NAMES,\n",
    "    satellite_channels_index=SELECTED_SATELLITE_CHANNELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb674aa9-7f12-40e3-8ea7-a17e19e938bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_nanmin(tensor, dim):\n",
    "    return torch.from_numpy(np.nanmin(tensor.cpu(), axis=dim)).to(device=tensor.device)\n",
    "\n",
    "\n",
    "def tensor_nanmax(tensor, dim):\n",
    "    return torch.from_numpy(np.nanmax(tensor.cpu(), axis=dim)).to(device=tensor.device)\n",
    "\n",
    "\n",
    "def rescale_tensors_to_0_to_1(tensors: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
    "    \"\"\"Rescales multiple tensors using the same min and max across all tensors.\n",
    "    \n",
    "    Args:\n",
    "        tensors: A dictionary of Tensors.  The dictionary keys should be the name of each tensor.\n",
    "            The values of dictionary should be Tensors of shape [batch_size, length].  All tensors\n",
    "            must have the same batch_size.\n",
    "            \n",
    "    Returns:\n",
    "        rescaled_tensors: A dict with the same keys as the input `tensors` dict but where each\n",
    "            tensor has had its values rescaled to be in the range [0, 1].\n",
    "    \"\"\"\n",
    "   \n",
    "    # Compute the maximum and the range, across all teh tensors.\n",
    "    list_of_tensors = list(tensors.values())\n",
    "    tensors_concatenated = torch.cat(list_of_tensors, dim=1)\n",
    "    del list_of_tensors\n",
    "    minimum = tensor_nanmin(tensors_concatenated, dim=1)\n",
    "    maximum = tensor_nanmax(tensors_concatenated, dim=1)\n",
    "    min_max_range = maximum - minimum\n",
    "    del maximum\n",
    "\n",
    "    minimum = minimum.unsqueeze(-1)\n",
    "    min_max_range = min_max_range.unsqueeze(-1)\n",
    "    \n",
    "    # Rescale each tensor\n",
    "    rescaled_tensors = {}\n",
    "    for name, tensor in tensors.items():\n",
    "        rescaled_tensors[name] = (tensor - minimum) / min_max_range\n",
    "        \n",
    "    return rescaled_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b4c9494-b343-409f-b922-077393f7181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fourier_features(array: torch.Tensor, n_fourier_features: int = 4, min_freq: float = 1, max_freq: float = 4) -> torch.Tensor:\n",
    "    \"\"\"Compute fourier features for a single dimension, across all examples in a batch.\n",
    "    \n",
    "    Adapted from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \n",
    "    Args:\n",
    "        array: Tensor of shape [batch_size, length] with values in the range [0, 1].\n",
    "            For the time dimension, `length` will be the timeseries sequence length.\n",
    "            For spatial dimensions, `length` will be the width or height.\n",
    "        n_fourer_features: Total number of requested fourier features\n",
    "        \n",
    "    Returns:\n",
    "        fourer_features: A tensor of the same dtype and device as `array`,\n",
    "            with shape [batch_size, length, n_fourier_features].  Fourier features with even indexes\n",
    "            are sine.  Odd indexes are cosine.\n",
    "    \"\"\"\n",
    "    #assert np.nanmax(array) <= 1\n",
    "    #assert np.nanmin(array) >= 0\n",
    "    assert n_fourier_features % 2 == 0\n",
    "    assert min_freq > 0\n",
    "    assert max_freq > min_freq\n",
    "\n",
    "    batch_size, length = array.shape[:2]\n",
    "    array = array * np.pi / 2\n",
    "    array = array.unsqueeze(-1)\n",
    "    div_term = torch.linspace(start=min_freq, end=max_freq, steps=n_fourier_features // 2, dtype=array.dtype, device=array.device)\n",
    "    fourier_features = torch.full(\n",
    "        size=(batch_size, length, n_fourier_features), \n",
    "        fill_value=np.NaN, \n",
    "        dtype=array.dtype, \n",
    "        device=array.device,\n",
    "    )\n",
    "\n",
    "    fourier_features[:, :, 0::2] = torch.sin(array * div_term)\n",
    "    fourier_features[:, :, 1::2] = torch.cos(array * div_term)\n",
    "    return fourier_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c7e31d3-2ae1-49b8-9cf2-526c4a2e3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_and_temporal_fourier_features(\n",
    "    batch: dict[str, torch.Tensor], \n",
    "    num_fourier_time_features: int = 4,\n",
    "    num_fourier_spatial_features_total: int = 8,  # Total for both height and width\n",
    ") -> None:\n",
    "    for dim_name in ('_x_coords', '_y_coords', 'time'):\n",
    "        all_coords = {key: tensor for key, tensor in batch.items() if key.endswith(dim_name)}\n",
    "        if f\"gsp{dim_name}\" in all_coords and dim_name != \"time\":\n",
    "            # We're only selecting a single GSP.  So GSP x_coords and y_coords are 1D.  We want 2D.\n",
    "            all_coords[f\"gsp{dim_name}\"] = all_coords[f\"gsp{dim_name}\"].unsqueeze(-1)\n",
    "        all_coords = rescale_tensors_to_0_to_1(all_coords)\n",
    "        if dim_name == \"time\":\n",
    "            n_fourier_features = num_fourier_time_features\n",
    "        else:\n",
    "            n_fourier_features = num_fourier_spatial_features_total // 2\n",
    "        for key, coords in all_coords.items():\n",
    "            batch[f\"{key}_fourier_features\"] = compute_fourier_features(\n",
    "                coords,\n",
    "                n_fourier_features=n_fourier_features,\n",
    "                min_freq=4,\n",
    "                max_freq=32,\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f76d1934-729a-4379-a899-85c9011aba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imagery(tensor, example_i, channel_i):\n",
    "    fig, axes = plt.subplots(ncols=4, figsize=(20, 10))\n",
    "    # axes = list(chain.from_iterable(axes))\n",
    "    # \"example\", \"time_index\", \"channels_index\", \"y_index\", \"x_index\"\n",
    "    data_to_plot = tensor[example_i, :4, channel_i, :, :]\n",
    "    vmin = data_to_plot.min()\n",
    "    vmax = data_to_plot.max()\n",
    "    for time_index, ax in enumerate(axes):\n",
    "        ax.imshow(data_to_plot[time_index].numpy()[::-1], vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(time_index)\n",
    "    return fig, axes\n",
    "\n",
    "#EXAMPLE_I = 1\n",
    "#CHANNEL_I = 7\n",
    "#plot_imagery(batch[\"opticalflow\"], example_i=EXAMPLE_I, channel_i=CHANNEL_I);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f668ab-42e8-43dd-b0a8-db441509b720",
   "metadata": {},
   "source": [
    "## Put the dataset into a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70700409-7e46-4992-8de3-3e99ec16deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_kwargs = dict(\n",
    "    batch_size=None,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d24af29-69b5-49f4-9825-d85c9f4c491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    **dataloader_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "412ce4fe-e39e-4b9e-97a6-dd15a831b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = data.DataLoader(\n",
    "    SimpleNowcastingDataset(\n",
    "        data_path=DATA_PATH / \"test\",\n",
    "        data_source_names=DATA_SOURCE_NAMES,\n",
    "        satellite_channels_index=SELECTED_SATELLITE_CHANNELS,\n",
    "    ),\n",
    "    **dataloader_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840802e2-c9e5-4bd9-9fde-c1ca326b2a62",
   "metadata": {},
   "source": [
    "# Define the ML model\n",
    "\n",
    "Each input \"row\" to the Perceiver will look like this:\n",
    "\n",
    "Shape: batch_size, seq_len, embedding_dim\n",
    "\n",
    "Try to keep the constant-length things in the same position.  e.g. the modality embedding always comes first.  The position encoding always comes second (for modalities which have spatial positions).\n",
    "\n",
    "Plan for Friday:\n",
    "\n",
    "* Opticalflow: (just one timestep at a time)\n",
    "    - relative temporal encoding for that timestep of imagery (4)\n",
    "    - relative x and y position encoding (8)\n",
    "    - 4x4 patch of pixel values (16)\n",
    "    - learnt channel embedding (4)\n",
    "    - learnt modality identifier (8)\n",
    "* PV: (mean across PV systems)\n",
    "    - relative temporal encoding of t0 (4)\n",
    "    - entire historical timeseries for mean across PV systems (7)\n",
    "    - learnt modality identifier (29)\n",
    "* NWP (mean across x and y positions):\n",
    "    - relative temporal encoding of t0 (4)\n",
    "    - entire timeseries for one channel (4)\n",
    "    - learnt NWP channel embedding (4)\n",
    "    - learnt modality identifier (28)\n",
    "* Datetime:\n",
    "    - all timesteps for forecast (4) x 9 features\n",
    "    - learnt modality embedding (4)\n",
    "* GSP ID:\n",
    "    - embedding of GSP ID\n",
    "    - learnt modality identifier\n",
    "* Query:\n",
    "    - learnt query\n",
    "\n",
    "\n",
    "Plan for Monday:\n",
    "\n",
    "All position encodings should capture whether it's within the GSP region or not.\n",
    "\n",
    "* PV: (individual PV systems)\n",
    "    - relative temporal encoding of t0 (4)\n",
    "    - relative x and y position encoding (8)\n",
    "    - entire historical timeseries for one PV system (7)\n",
    "    - learnt PV ID embedding (10)\n",
    "    - learnt modality identifier (11)\n",
    "\n",
    "* The learnt \"query\" to the Perceiver:\n",
    "    - Embedding of the GSP ID?  Or different learnable params, one for each GSP ID?  Or just a single learnt number?\n",
    "* GSP ID & area\n",
    "    - **area** (i.e. how big is it?)\n",
    "    - embedding of GSP ID\n",
    "    - learnt modality identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0e23ac6-aed9-4438-9b20-aac8227bb0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixture density network\n",
    "\n",
    "\n",
    "PI = 'PI'\n",
    "MU = 'MU'\n",
    "SIGMA = 'SIGMA'\n",
    "MEAN = 'MEAN'\n",
    "ATTENTION_WEIGHTS = 'ATTENTION_WEIGHTS'\n",
    "\n",
    "class MixtureDensityNetwork(nn.Module):\n",
    "    def __init__(self, in_features: int, num_gaussians: int = 2):\n",
    "        super().__init__()\n",
    "        self.pi = nn.Linear(in_features=in_features, out_features=num_gaussians)\n",
    "        self.mu = nn.Linear(in_features=in_features, out_features=num_gaussians)\n",
    "        self.sigma = nn.Linear(in_features=in_features, out_features=num_gaussians)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        pi = self.pi(x)\n",
    "        pi = F.softmax(pi, dim=-1)\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        sigma = torch.exp(sigma)\n",
    "        return {PI: pi, MU: mu, SIGMA: sigma}\n",
    "    \n",
    "    \n",
    "def get_distribution(\n",
    "    network_output: dict[str, torch.Tensor], \n",
    "    example_i: Optional[int] = None\n",
    ") -> torch.distributions.MixtureSameFamily:\n",
    "\n",
    "    pi = network_output[PI]\n",
    "    mu = network_output[MU]\n",
    "    sigma = network_output[SIGMA]\n",
    "    \n",
    "    if example_i is not None:\n",
    "        pi = pi[example_i]\n",
    "        mu = mu[example_i]\n",
    "        sigma = sigma[example_i]\n",
    "\n",
    "    mixture_distribution = torch.distributions.Categorical(probs=pi)\n",
    "    component_distribution = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    gaussian_mixture_model = torch.distributions.MixtureSameFamily(mixture_distribution, component_distribution)\n",
    "    return gaussian_mixture_model\n",
    "\n",
    "\n",
    "\n",
    "def plot_probs(pi, mu, sigma, ax, left, right, example_i=0):\n",
    "    sweep_n_steps = 100\n",
    "    sweep_start = 1\n",
    "    sweep_stop = 0\n",
    "       \n",
    "    n_timesteps = pi.shape[1]\n",
    "\n",
    "    # Define a 'sweep' matrix which we pass into log_prob to get probabilities \n",
    "    # for a range of values at each timestep.  Those values range from sweep_start to sweep_stop\n",
    "    sweep = torch.linspace(start=sweep_start, end=sweep_stop, steps=sweep_n_steps, dtype=torch.float32, device=pi.device)\n",
    "    sweep = sweep.unsqueeze(-1).expand(sweep_n_steps, n_timesteps)\n",
    "    \n",
    "    # Get probabilities.\n",
    "    distribution = get_distribution({PI: pi, MU: mu, SIGMA: sigma}, example_i=example_i)\n",
    "    log_probs = distribution.log_prob(sweep)\n",
    "    probs = torch.exp(log_probs).detach().cpu().numpy()\n",
    "    \n",
    "    # Plot!\n",
    "    extent = (left, right, sweep_stop, sweep_start) # left, right, bottom, top\n",
    "    ax.imshow(probs, aspect='auto', interpolation='none', extent=extent, cmap='Greys')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4ecccdf-23a0-4b3f-8256-13847c9c7c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(batch: dict[str, torch.Tensor], network_output: dict[str, torch.Tensor]) -> plt.Figure:\n",
    "    actual = batch[\"gsp\"].detach().cpu()\n",
    "    \n",
    "    # get the mean prediction\n",
    "    mu = network_output[MU]\n",
    "    pi = network_output[PI]\n",
    "    predicted = torch.sum((mu * pi), dim=-1).squeeze().detach().cpu()\n",
    "\n",
    "    gsp_id = batch[\"gsp_id\"].squeeze().detach().cpu()\n",
    "    historical_pv = batch[\"pv\"].squeeze().detach().cpu().numpy()\n",
    "    if \"nwp\" in batch:\n",
    "        nwp_chan = NWP_CHANNELS.index(\"dswrf\")\n",
    "        nwp = batch[\"nwp\"][:, :, nwp_chan].mean(dim=[2, 3]).detach().cpu().numpy()\n",
    "        nwp_time = batch[\"nwp_time\"].squeeze().detach().cpu().numpy()\n",
    "    t0_datetimes = batch[\"t0_datetime_UNIX_epoch\"].squeeze().detach().cpu().numpy()\n",
    "    t0_datetimes = pd.to_datetime(t0_datetimes, unit='s')\n",
    "    fig, axes = plt.subplots(nrows=8, ncols=4, figsize=(20, 20))\n",
    "    axes = list(chain.from_iterable(axes))\n",
    "    FIFTEEN_MINUTES = (15 / (60 * 24))\n",
    "    for example_i, ax in enumerate(axes):\n",
    "        t0_datetime = t0_datetimes[example_i]\n",
    "        t1_datetime = t0_datetime + pd.Timedelta(\"30 minutes\")\n",
    "        forecast_datetimes = pd.date_range(t1_datetime, periods=4, freq=\"30 min\")\n",
    "\n",
    "        # Plot historical PV yield\n",
    "        historical_pv_datetimes = pd.date_range(t0_datetime - pd.Timedelta(\"30 minutes\"), periods=7, freq=\"5 min\")\n",
    "        plot_probs(\n",
    "            pi=network_output[PI],\n",
    "            mu=network_output[MU],\n",
    "            sigma=network_output[SIGMA],\n",
    "            ax=ax,\n",
    "            left=mdates.date2num(forecast_datetimes[0]) - FIFTEEN_MINUTES,\n",
    "            right=mdates.date2num(forecast_datetimes[-1]) + FIFTEEN_MINUTES,\n",
    "            example_i=example_i,\n",
    "        )\n",
    "        ax.plot(\n",
    "            historical_pv_datetimes,\n",
    "            historical_pv[example_i],\n",
    "            color=\"grey\",\n",
    "            alpha=0.5\n",
    "        )\n",
    "        ax.plot(\n",
    "            historical_pv_datetimes,\n",
    "            np.nanmean(historical_pv, axis=2)[example_i],\n",
    "            label=\"Historical mean PV\",\n",
    "            linewidth=3,\n",
    "            alpha=0.8,\n",
    "            color=\"red\",\n",
    "        )\n",
    "        \n",
    "        # Plot prediction for GSP PV yield and actual GSP PV yield\n",
    "        ax.plot(forecast_datetimes, predicted[example_i], label=\"Predicted GSP PV\", color=\"orange\", linewidth=3, alpha=0.8)\n",
    "        ax.plot(forecast_datetimes, actual[example_i], label=\"Actual GSP PV\", linewidth=3, alpha=0.8)\n",
    "        \n",
    "        # Plot NWP params:\n",
    "        if \"nwp\" in batch:\n",
    "            ax2 = ax.twinx()\n",
    "            nwp_time_for_example = pd.to_datetime(nwp_time[example_i], unit=\"s\")\n",
    "            ax2.plot(nwp_time_for_example, nwp[example_i], label=\"NWP irradiance\", color=\"green\", alpha=0.8)\n",
    "            ax2.yaxis.set_ticks([])\n",
    "            ax2.set_ylim(-2, 2)\n",
    "        \n",
    "        # Formatting\n",
    "        ax.xaxis.set_major_locator(mdates.HourLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\"))\n",
    "        ax.set_title(\"GSP {:.0f} on {}\".format(gsp_id[example_i], t0_datetime.strftime(\"%Y-%m-%d\")), y=0.8)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_xlim(mdates.date2num(historical_pv_datetimes[0]), mdates.date2num(forecast_datetimes[-1]))\n",
    "        if example_i == 0:\n",
    "            fig.legend(framealpha=0, loc=\"center right\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a72eaab7-6781-44e1-8171-bf768d374bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SATELLITE_IMAGE_SIZE_PIXELS = 24\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    \n",
    "    # 2. **********\n",
    "    # list of results dataframes. This is used to save validation results\n",
    "    results_dfs = []\n",
    "    # **********\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        embed_dim_query: int = 512,  # TODO: Play with these numbers\n",
    "        num_elements_query: int = 32,\n",
    "        num_heads: int = 32,\n",
    "        num_latent_transformer_encoder_layers: int = 4,\n",
    "        num_elements_into_output_layers: Optional[int] = None,  #: The subset of elements to take from the latent query into the output layers.  If None then the same as num_elements_query\n",
    "        dropout: float = 0.0,\n",
    "        byte_array_dim: int = 64,  # The dimensionality of each key & value element.\n",
    "        gsp_id_embedding_dim: int = 16,\n",
    "        num_fourier_time_features: int = 4,\n",
    "        num_fourier_spatial_features: int = 8,  # Total for both height and width\n",
    "        satellite_patch_size_per_dim: int = 2,  # The total patch size will be satellite_patch_size_per_dim x satellite_patch_size_per_dim\n",
    "        satellite_channel_embedding_dim: int = 4,  # The size of the embedding of the satellite channel ID\n",
    "        nwp_channel_embedding_dim: int = 4,\n",
    "        share_weights_across_latent_transformer_layers: bool = True,\n",
    "        pv_system_id_embedding_dim: int = 16,\n",
    "        results_file_name: str = \"results.csv\",\n",
    "        num_gaussians: int = 2,\n",
    "        num_examples_per_batch_to_drop_1_modality: int = 10,  # Perceiver paper droped out video with 30% probability (32 x 0.3 ~ 10).\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim_query = embed_dim_query\n",
    "        self.num_elements_query = num_elements_query\n",
    "        self.num_fourier_time_features = num_fourier_time_features\n",
    "        self.num_fourier_spatial_features = num_fourier_spatial_features\n",
    "        self.satellite_patch_size_per_dim = satellite_patch_size_per_dim\n",
    "        self.results_file_name = results_file_name\n",
    "        self.gsp_id_embedding_dim = gsp_id_embedding_dim\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.num_examples_per_batch_to_drop_1_modality = num_examples_per_batch_to_drop_1_modality\n",
    "        \n",
    "        if num_elements_into_output_layers is None:\n",
    "            num_elements_into_output_layers = num_elements_query\n",
    "        self.num_elements_into_output_layers = num_elements_into_output_layers\n",
    "        \n",
    "        assert num_elements_into_output_layers <= num_elements_query\n",
    "\n",
    "        #self.query_norm1 = nn.LayerNorm(embed_dim_query)\n",
    "        #self.query_norm2 = nn.LayerNorm(embed_dim_query)\n",
    "        #self.byte_array_norm = nn.LayerNorm(byte_array_dim)\n",
    "        \n",
    "        # PERCEIVER #################################\n",
    "        # Layers for the Perceiver model:\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim_query,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            kdim=byte_array_dim,\n",
    "            vdim=byte_array_dim\n",
    "        )\n",
    "        \n",
    "        transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim_query,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        if share_weights_across_latent_transformer_layers:\n",
    "            self.latent_transformer = nn.Sequential(\n",
    "                *[transformer_encoder_layer for _ in range (num_latent_transformer_encoder_layers)]\n",
    "            )\n",
    "        else:\n",
    "            self.latent_transformer = nn.TransformerEncoder(\n",
    "                encoder_layer=transformer_encoder_layer,\n",
    "                num_layers=num_latent_transformer_encoder_layers,\n",
    "            )\n",
    "\n",
    "        # Feed forwards output layers #################################\n",
    "        self.output_layers = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=num_elements_into_output_layers * embed_dim_query, \n",
    "                out_features=num_elements_into_output_layers),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.mixture_density_network = MixtureDensityNetwork(\n",
    "            in_features=num_elements_into_output_layers,\n",
    "            num_gaussians=num_gaussians,\n",
    "        )\n",
    "        \n",
    "        self.gsp_id_embedding = nn.Embedding(\n",
    "            num_embeddings=MAX_GSP_ID + 1,\n",
    "            embedding_dim=gsp_id_embedding_dim,\n",
    "        )\n",
    "        \n",
    "        self.gsp_id_embedding_query_generator = nn.Sequential(\n",
    "            nn.Linear(in_features=gsp_id_embedding_dim, out_features=gsp_id_embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=gsp_id_embedding_dim * 2, out_features=gsp_id_embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=gsp_id_embedding_dim * 2, out_features=gsp_id_embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.time_fourier_features_query_generator = nn.Sequential(\n",
    "            nn.Linear(in_features=num_fourier_time_features, out_features=num_fourier_time_features * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=num_fourier_time_features * 2, out_features=num_fourier_time_features * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=num_fourier_time_features * 2, out_features=num_fourier_time_features),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.gsp_id_embedding_key_generator = nn.Sequential(\n",
    "            nn.Linear(in_features=gsp_id_embedding_dim, out_features=gsp_id_embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=gsp_id_embedding_dim * 2, out_features=gsp_id_embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=gsp_id_embedding_dim * 2, out_features=gsp_id_embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.satellite_chan_embedding = nn.Embedding(\n",
    "            num_embeddings=len(SATELLITE_CHANNELS),\n",
    "            embedding_dim=satellite_channel_embedding_dim,\n",
    "        )\n",
    "        self.nwp_chan_embedding = nn.Embedding(\n",
    "            num_embeddings=len(NWP_CHANNELS),\n",
    "            embedding_dim=nwp_channel_embedding_dim,\n",
    "        )\n",
    "        self.pv_system_id_embedding = nn.Embedding(\n",
    "            num_embeddings=max(pv_system_id_index_lut) + 1,\n",
    "            embedding_dim=pv_system_id_embedding_dim,\n",
    "        )\n",
    "        \n",
    "        # Learnable parametrs\n",
    "        self.register_buffer(\n",
    "            \"query\", \n",
    "            nn.Parameter(\n",
    "                torch.randn(\n",
    "                    num_elements_query, \n",
    "                    embed_dim_query\n",
    "                    - gsp_id_embedding_dim\n",
    "                    - num_fourier_time_features\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"learnt_modality_identifier_for_optical_flow\", \n",
    "            # 36 = 4x4 patch +  + 4 channel embedding\n",
    "            nn.Parameter(\n",
    "                torch.randn(\n",
    "                    byte_array_dim \n",
    "                    - num_fourier_time_features\n",
    "                    - num_fourier_spatial_features\n",
    "                    - gsp_id_embedding_dim # 16\n",
    "                    - (satellite_patch_size_per_dim ** 2) \n",
    "                    - satellite_channel_embedding_dim\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"learnt_modality_identifier_for_pv_yield\",\n",
    "            # 7 timesteps of PV data\n",
    "            nn.Parameter(\n",
    "                torch.randn(\n",
    "                    byte_array_dim   # 64\n",
    "                    - num_fourier_time_features # 4\n",
    "                    - num_fourier_spatial_features # 8\n",
    "                    - gsp_id_embedding_dim # 16\n",
    "                    - pv_system_id_embedding_dim # 16\n",
    "                    - 7\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"learnt_modality_identifier_for_nwp\",\n",
    "            # 4 timesteps of NWP data\n",
    "            nn.Parameter(torch.randn(byte_array_dim - num_fourier_time_features - 4 - nwp_channel_embedding_dim))\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"learnt_modality_identifier_for_t0_datetime_features\", \n",
    "            nn.Parameter(torch.randn(byte_array_dim - N_DATETIME_FEATURES))\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"learnt_modality_identifier_for_gsp_id\", \n",
    "            nn.Parameter(torch.randn(byte_array_dim - gsp_id_embedding_dim))\n",
    "        )\n",
    "\n",
    "\n",
    "        #self.query_generator = nn.Sequential(\n",
    "        #    nn.Linear(in_features=16, out_features=16),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(in_features=16, out_features=16),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(in_features=16, out_features=num_elements_query * embed_dim_query),\n",
    "        #    nn.ReLU(),\n",
    "        #)\n",
    "        \n",
    "    def forward(self, batch: dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: Contains these keys:\n",
    "                - opticalflow: Shape: batch_size, time, \"channels_index\", \"y_index\", \"x_index\"\n",
    "                \n",
    "        Returns:\n",
    "            Tensor of shape batch_size, time\n",
    "        \"\"\"\n",
    "        # byte_arrays is the list of modalities to be concatenated and fed into the perceiver\n",
    "        byte_arrays: list[torch.Tensor] = []\n",
    "\n",
    "        #######################################################\n",
    "        # GENERATE POSITION ENCODING\n",
    "        get_spatial_and_temporal_fourier_features(\n",
    "            batch, \n",
    "            num_fourier_time_features=self.num_fourier_time_features,\n",
    "            num_fourier_spatial_features_total=self.num_fourier_spatial_features,  # Total for both height and width\n",
    "        )\n",
    "        # batch now has keys like \"pv_x_coords_fourier_features\"\n",
    "        \n",
    "        num_elements_per_modality: list[int] = []\n",
    "        \n",
    "        #######################################################\n",
    "        # GET DATA INTO THE RIGHT SHAPE #######################\n",
    "        # satellite data starts with shape \"example\", \"time_index\", \"channels_index\", \"y_index\", \"x_index\"\n",
    "        if \"opticalflow\" in batch:\n",
    "            opticalflow = batch[\"opticalflow\"]\n",
    "            original_batch_size, original_satellite_seq_len = opticalflow.shape[:2]\n",
    "\n",
    "            # Reshape so each timestep is seen as a separate example\n",
    "            opticalflow = einops.rearrange(opticalflow, \"b t c h w -> (b t) c h w\")\n",
    "\n",
    "            # Take patches of pixels:\n",
    "            # Adapted from https://github.com/openclimatefix/satflow/blob/main/satflow/models/utils.py#L54\n",
    "            opticalflow = einops.rearrange(\n",
    "                opticalflow, \n",
    "                \"b c (h dh) (w dw) -> b c h w (dh dw)\", \n",
    "                dh=self.satellite_patch_size_per_dim, \n",
    "                dw=self.satellite_patch_size_per_dim,\n",
    "            )\n",
    "\n",
    "            b, c, h, w, d = opticalflow.shape\n",
    "            \n",
    "            # Do the same for the temporal & spatial encoding\n",
    "            \n",
    "            # Reshape so each timestep is seen as a separate example\n",
    "            batch[\"opticalflow_time_fourier_features\"] = einops.rearrange(\n",
    "                batch[\"opticalflow_time_fourier_features\"], \n",
    "                \"b t n_fourier_features -> (b t) n_fourier_features\",\n",
    "            )\n",
    "            \n",
    "            # Take patches of pixels:\n",
    "            for dim_name in (\"x_coords\", \"y_coords\"):\n",
    "                key_name = f\"opticalflow_{dim_name}_fourier_features\"\n",
    "                \n",
    "                batch[key_name] = einops.reduce(\n",
    "                    batch[key_name], \n",
    "                    \"b (length dl) n_fourier_features -> b length n_fourier_features\",\n",
    "                    reduction=\"mean\",\n",
    "                    dl=self.satellite_patch_size_per_dim,\n",
    "                )\n",
    "                \n",
    "                # Repeat\n",
    "                batch[key_name] = torch.repeat_interleave(batch[key_name], repeats=original_satellite_seq_len, dim=0)\n",
    "                \n",
    "            # GSP ID per pixel\n",
    "            gsp_id_per_pixel = batch[\"gsp_id_per_satellite_pixel\"]\n",
    "            middle_index = (self.satellite_patch_size_per_dim**2 // 2) + (self.satellite_patch_size_per_dim // 2)\n",
    "            if middle_index > 0:\n",
    "                middle_index -= 1\n",
    "            gsp_id_per_pixel = einops.rearrange(\n",
    "                gsp_id_per_pixel,\n",
    "                \"b (h dh) (w dw) -> b h w (dh dw)\",\n",
    "                dh=self.satellite_patch_size_per_dim, \n",
    "                dw=self.satellite_patch_size_per_dim,\n",
    "            )[..., middle_index]\n",
    "            gsp_id_per_pixel = self.gsp_id_embedding_key_generator(self.gsp_id_embedding(gsp_id_per_pixel))\n",
    "            gsp_id_per_pixel = torch.repeat_interleave(gsp_id_per_pixel, repeats=original_satellite_seq_len, dim=0)\n",
    "\n",
    "            # Encode the channel\n",
    "            sat_chan_list = torch.arange(c, device=self.device)\n",
    "            embedded_sat_chan_list = self.satellite_chan_embedding(sat_chan_list)    \n",
    "\n",
    "            # Concat position encoding for the height and width:\n",
    "            opticalflow = torch.cat(\n",
    "                (\n",
    "                    einops.repeat(batch[\"opticalflow_time_fourier_features\"], \"b n_fourier_features -> b c h w n_fourier_features\", b=b, c=c, h=h, w=w),\n",
    "                    # Position encode the height:\n",
    "                    # (\"d\" is the dimensionality of the Perceiver model)\n",
    "                    # Position encode the width:\n",
    "                    einops.repeat(batch[\"opticalflow_x_coords_fourier_features\"], \"b w d -> b c h w d\", b=b, c=c, h=h),\n",
    "                    einops.repeat(batch[\"opticalflow_y_coords_fourier_features\"], \"b h d -> b c h w d\", b=b, c=c, w=w),\n",
    "                    einops.repeat(\n",
    "                        gsp_id_per_pixel,\n",
    "                        \"b h w d-> b c h w d\", b=b, c=c, h=h, w=w,\n",
    "                    ),\n",
    "                    opticalflow,\n",
    "                    einops.repeat(embedded_sat_chan_list, \"c d -> b c h w d\", b=b, c=c, h=h, w=w),\n",
    "                    einops.repeat(self.learnt_modality_identifier_for_optical_flow, \"d -> b c h w d\", b=b, c=c, h=h, w=w),\n",
    "                ),\n",
    "                dim=-1,\n",
    "            )\n",
    "\n",
    "            # opticalflow now has shape: b, c, h, w, perceiver element dim\n",
    "            # e.g. 128, 10, 6, 6, 32\n",
    "\n",
    "            # Reshape into batch_size, perceiver element, perceiver element dim, ready for the perceiver\n",
    "            opticalflow = einops.rearrange(opticalflow, \"b c h w d -> b (c h w) d\")\n",
    "            num_elements_per_modality.append(opticalflow.shape[1])\n",
    "            byte_arrays.append(opticalflow)\n",
    "        \n",
    "        #######################################################\n",
    "        # CROSS ATTENTION! ####################################\n",
    "        \n",
    "        # Create query\n",
    "        # num_elements is the number of elements in the set that's fed into cross attention\n",
    "        # Embed the GSP ID \n",
    "        ##query = self.gsp_id_embedding(batch[\"gsp_id\"])\n",
    "        # Then put the embedding through a linear layer so the model can learn different queries\n",
    "        # for each element of the Transformer's input.\n",
    "        ##query = self.query_generator(query)\n",
    "        # Repeat for each timestep:\n",
    "        ##query = torch.repeat_interleave(query, repeats=original_satellite_seq_len, dim=0)\n",
    "        ##query = query.squeeze()\n",
    "        ##query = einops.repeat(\n",
    "        ##    query, \n",
    "        ##    \"b (num_elements embed_dim) -> b num_elements embed_dim\", \n",
    "        ##    b=b, \n",
    "        ##    num_elements=self.num_elements_query,\n",
    "        ##    embed_dim=self.embed_dim_query,\n",
    "        ##)\n",
    "        \n",
    "        # GSP ID embedding\n",
    "        # TODO: Reduce reduncancy between this and datetime features.\n",
    "        #gsp_embedding = self.gsp_id_embedding(batch[\"gsp_id\"])\n",
    "        #gsp_embedding = torch.cat(\n",
    "        #    (\n",
    "        #        gsp_embedding,\n",
    "        #        einops.repeat(\n",
    "        #            self.learnt_modality_identifier_for_gsp_id,\n",
    "        #            \"d -> original_batch_size d\",\n",
    "        #            original_batch_size=original_batch_size,\n",
    "        #        ),\n",
    "        #    ), \n",
    "        #    dim=-1,\n",
    "        #)\n",
    "        #gsp_embedding = torch.repeat_interleave(gsp_embedding, repeats=original_satellite_seq_len, dim=0)\n",
    "        #byte_arrays.append(gsp_embedding.unsqueeze(1))\n",
    "        \n",
    "        # Datetime features\n",
    "        #t0_datetime_features = torch.cat(\n",
    "        #    (\n",
    "        #        batch[\"t0_datetime_features\"],\n",
    "        #        einops.repeat(\n",
    "        #            self.learnt_modality_identifier_for_t0_datetime_features,\n",
    "        #            \"d -> original_batch_size d\",\n",
    "        #            original_batch_size=original_batch_size,\n",
    "        #        ),\n",
    "        #    ),\n",
    "        #    dim=-1,\n",
    "        #)\n",
    "        #t0_datetime_features = torch.repeat_interleave(t0_datetime_features, repeats=original_satellite_seq_len, dim=0)\n",
    "        #byte_arrays.append(t0_datetime_features.unsqueeze(1))\n",
    "\n",
    "        # Historical PV data\n",
    "        # input shape: batch=32, timestep=7, pv system=128\n",
    "        # output shape: batch=128, n_pv_systems, d_model\n",
    "        key_padding_mask = None\n",
    "        if \"pv\" in batch:\n",
    "            pv_yield = batch[\"pv\"]\n",
    "            pv_yield = einops.rearrange(pv_yield, \"b t pv_system -> b pv_system t\")\n",
    "            n_pv_systems = pv_yield.shape[1]\n",
    "            #RuntimeError: Sizes of tensors must match except in dimension 2. Expected size 1 but got size 128 for tensor number 1 in the list.\n",
    "            pv_yield = torch.cat(\n",
    "                (\n",
    "                    # fourier_time_features[\"pv_time\"] is a tensor of shape [batch_size, seq_length, n_fourier_features]\n",
    "                    einops.repeat(\n",
    "                        batch[\"pv_time_fourier_features\"][:, -2:-1],  # Just take the t0 timestep\n",
    "                        \"b 1 d -> b n_pv_systems d\",\n",
    "                        n_pv_systems=n_pv_systems,\n",
    "                    ),\n",
    "                    batch[\"pv_x_coords_fourier_features\"],\n",
    "                    batch[\"pv_y_coords_fourier_features\"],\n",
    "                    self.gsp_id_embedding_key_generator(\n",
    "                        self.gsp_id_embedding(batch[\"pv_gsp_id\"].to(torch.int32)),\n",
    "                    ),\n",
    "                    pv_yield,\n",
    "                    # Convert pv_system_id to ints for embedding\n",
    "                    self.pv_system_id_embedding(\n",
    "                        batch[\"pv_system_id_index\"].to(torch.int32),\n",
    "                    ),\n",
    "                    einops.repeat(\n",
    "                        self.learnt_modality_identifier_for_pv_yield,\n",
    "                        \"d -> original_batch_size n_pv_systems d\",\n",
    "                        original_batch_size=original_batch_size,\n",
    "                        n_pv_systems=n_pv_systems,\n",
    "                    ),\n",
    "                ),\n",
    "                dim=-1,\n",
    "            )\n",
    "            # We will mask missing PV systems later\n",
    "            pv_yield = pv_yield.float()\n",
    "            pv_yield = torch.nan_to_num(pv_yield, nan=-1)\n",
    "            pv_yield = torch.repeat_interleave(pv_yield, repeats=original_satellite_seq_len, dim=0)\n",
    "            key_padding_mask = torch.repeat_interleave(batch[\"pv\"].isnan().any(dim=1), repeats=original_satellite_seq_len, dim=0)\n",
    "            num_elements_per_modality.insert(0, pv_yield.shape[1])\n",
    "            byte_arrays.insert(0, pv_yield)\n",
    "        \n",
    "        # NWP data\n",
    "        # Original shape: \"example\", \"time_index\", \"channels_index\", \"y_index\", \"x_index\"\n",
    "        # We want to end up with each element being all timesteps for a single channel, taking the mean across x and y:\n",
    "        if \"nwp\" in batch:\n",
    "            nwp = batch[\"nwp\"].mean(dim=[-2, -1])  # take the mean across x and y\n",
    "            nwp = einops.rearrange(nwp, \"b t c -> b c t\")\n",
    "            nwp_b, nwp_c, nwp_t = nwp.shape\n",
    "            # Encode the channel\n",
    "            nwp_chan_list = torch.arange(nwp_c, device=self.device)\n",
    "            embedded_nwp_chan_list = self.nwp_chan_embedding(nwp_chan_list)\n",
    "            embedded_nwp_chan_list = einops.repeat(\n",
    "                embedded_nwp_chan_list, \n",
    "                \"c d -> b c d\", b=nwp_b, c=nwp_c)\n",
    "            # RuntimeError: Sizes of tensors must match except in dimension 2. Expected size 1 but got size 10 for tensor number 1 in the list.\n",
    "            nwp = torch.cat(\n",
    "                (\n",
    "                    # fourier_time_features[\"nwp_time\"] is a tensor of shape [batch_size, seq_length, n_fourier_features]\n",
    "                    einops.repeat(\n",
    "                        batch[\"nwp_time_fourier_features\"][:, -2:-1],  # Just take the last timestep\n",
    "                        \"b 1 d -> b c d\",\n",
    "                        c=nwp_c,\n",
    "                    ),\n",
    "                    nwp,\n",
    "                    embedded_nwp_chan_list,\n",
    "                    einops.repeat(\n",
    "                        self.learnt_modality_identifier_for_nwp,\n",
    "                        \"d -> original_batch_size c d\",\n",
    "                        original_batch_size=original_batch_size,\n",
    "                        c=nwp_c,\n",
    "                    ),\n",
    "                ),\n",
    "                dim=-1,\n",
    "            )\n",
    "            nwp = torch.repeat_interleave(nwp, repeats=original_satellite_seq_len, dim=0)\n",
    "            num_elements_per_modality.append(nwp.shape[1])\n",
    "            byte_arrays.append(nwp)\n",
    "\n",
    "        # Final preparation of data for Perceiver\n",
    "        # Shape of data going into perciever: [batch_size, perceiver element, perceiver element dim]\n",
    "        #         # RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 64 but got size 49 for tensor number 1 in the list.\n",
    "        byte_array = torch.cat(byte_arrays, dim=1)\n",
    "        \n",
    "        if key_padding_mask is not None and key_padding_mask.shape[1] < byte_array.shape[1]:\n",
    "            # If PV is present, then PV is always the first set of 128 elements (in the 2nd dim).\n",
    "            # We need to extend the key_padding_mask so it has the same number of elements as byte_array:\n",
    "            key_padding_mask = torch.cat(\n",
    "                (\n",
    "                    key_padding_mask,\n",
    "                    torch.full(\n",
    "                        size=(\n",
    "                            byte_array.shape[0], \n",
    "                            byte_array.shape[1] - key_padding_mask.shape[1]\n",
    "                        ), \n",
    "                        fill_value=False, dtype=torch.bool, device=byte_array.device),\n",
    "                ),\n",
    "                dim=1\n",
    "            )\n",
    "            \n",
    "        # Drop out 1 modality in self.num_examples_per_batch_to_drop_1_modality\n",
    "        if self.training and self.num_examples_per_batch_to_drop_1_modality > 0:\n",
    "            # add [1] for the last index\n",
    "            start_idx_of_each_modality = np.cumsum([0] + num_elements_per_modality)\n",
    "            num_modalities = len(num_elements_per_modality)\n",
    "            # Create a list of random example indexes where we want to drop out exactly one modality:\n",
    "            examples_to_dropout_one_modality = torch.randint(low=0, high=b, size=(self.num_examples_per_batch_to_drop_1_modality, ), device=byte_array.device)\n",
    "            # Pick which modalities to drop out in those examples:\n",
    "            modalities_to_drop = torch.randint(low=0, high=num_modalities, size=(self.num_examples_per_batch_to_drop_1_modality, ), device=byte_array.device)            \n",
    "            for i in range(self.num_examples_per_batch_to_drop_1_modality):\n",
    "                example_idx = examples_to_dropout_one_modality[i]\n",
    "                idx_of_modality_to_drop = modalities_to_drop[i]\n",
    "                if idx_of_modality_to_drop == 1:\n",
    "                    # quick hack to prevent us from dropping optical flow\n",
    "                    # (because we want the model to spend more time attending to optical flow).\n",
    "                    continue\n",
    "                start_idx = start_idx_of_each_modality[idx_of_modality_to_drop]\n",
    "                end_idx = start_idx_of_each_modality[idx_of_modality_to_drop + 1]\n",
    "                key_padding_mask[example_idx, start_idx:end_idx] = True\n",
    "                \n",
    "        # Query\n",
    "        query = einops.repeat(\n",
    "            self.query, \n",
    "            \"num_elements embed_dim -> b num_elements embed_dim\", \n",
    "            b=b, \n",
    "            num_elements=self.num_elements_query,\n",
    "        )\n",
    "        # Append the GSP ID and forecast_datetime\n",
    "        query = torch.cat(\n",
    "            (\n",
    "                query,\n",
    "                # The opticalflow time is the time that we want the forecast for.\n",
    "                einops.repeat(\n",
    "                    self.time_fourier_features_query_generator(\n",
    "                        batch[\"opticalflow_time_fourier_features\"],\n",
    "                    ),\n",
    "                    \"b d -> b num_elements d\",\n",
    "                    b=b,\n",
    "                    num_elements=self.num_elements_query,\n",
    "                    d=self.num_fourier_time_features\n",
    "                ),\n",
    "                einops.repeat(\n",
    "                    torch.repeat_interleave(\n",
    "                        self.gsp_id_embedding_query_generator(\n",
    "                            self.gsp_id_embedding(\n",
    "                                    torch.nan_to_num(\n",
    "                                        batch[\"gsp_id\"],\n",
    "                                        nan=MAX_GSP_ID,\n",
    "                                    ).to(torch.int32)\n",
    "                                ),\n",
    "                        ),\n",
    "                        repeats=original_satellite_seq_len,\n",
    "                        dim=0,\n",
    "                    ),\n",
    "                    \"b d -> b num_elements d\",\n",
    "                    b=b,\n",
    "                    num_elements=self.num_elements_query,\n",
    "                    d=self.gsp_id_embedding_dim\n",
    "                ),\n",
    "            ),\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        #query = self.query_norm1(query)\n",
    "        #byte_array = self.byte_array_norm(byte_array)\n",
    "        \n",
    "        attn_output, attn_weights = self.cross_attention(query, key=byte_array, value=byte_array, need_weights=True, key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        # LATENT TRANSFORMER\n",
    "        #attn_output = attn_output + query\n",
    "        #attn_output = self.query_norm2(attn_output)\n",
    "        attn_output = self.latent_transformer(attn_output)\n",
    "        \n",
    "        attn_output = attn_output[:, :self.num_elements_into_output_layers]\n",
    "        \n",
    "        # LINEAR LAYERS\n",
    "        out = self.output_layers(einops.rearrange(attn_output, \"b s d -> b (s d)\"))\n",
    "        out = self.mixture_density_network(out)\n",
    "        out = {\n",
    "            key: einops.rearrange(\n",
    "                tensor,\n",
    "                \"(b t) num_gaussians -> b t num_gaussians\", \n",
    "                b=original_batch_size, \n",
    "                t=original_satellite_seq_len,\n",
    "                num_gaussians=self.num_gaussians,\n",
    "            )\n",
    "            for key, tensor in out.items()\n",
    "        }\n",
    "        out[ATTENTION_WEIGHTS] = attn_weights\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _training_or_validation_step(self, batch: dict[str, torch.Tensor], batch_idx: int, tag: str) -> dict[str, object]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: The training or validation batch.  A dictionary.\n",
    "            tag: Either \"train\" or \"validation\"\n",
    "            batch_idx: The index of the batch.\n",
    "        \"\"\"\n",
    "        actual_gsp_power = batch[\"gsp\"]\n",
    "        network_output = self(batch)\n",
    "        distribution = get_distribution(network_output)\n",
    "        neg_log_prob_loss = torch.mean(-distribution.log_prob(actual_gsp_power))\n",
    "        \n",
    "        # get the mean prediction\n",
    "        mu = network_output[MU]\n",
    "        pi = network_output[PI]\n",
    "        predicted_gsp_power = torch.sum((mu * pi), dim=-1)\n",
    "        \n",
    "        mse_loss = F.mse_loss(predicted_gsp_power, actual_gsp_power)\n",
    "        nmae_loss = F.l1_loss(predicted_gsp_power, actual_gsp_power)\n",
    "        \n",
    "        self.log_dict(\n",
    "            {\n",
    "                f\"negative_log_probability/{tag}\": neg_log_prob_loss,\n",
    "                f\"MSE/{tag}\": mse_loss,\n",
    "                f\"NMAE/{tag}\": nmae_loss,\n",
    "            },\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            sync_dist=True,  # Required for distributed training (even multi-GPU on signle machine)\n",
    "        )\n",
    "\n",
    "        if batch_idx < 3:\n",
    "            # Log timeseries of actual GSP power and predicted GSP power\n",
    "            figure_name = f\"{tag}/plot/timeseries/epoch={self.current_epoch};batch_idx={batch_idx}\"\n",
    "            fig = plot_timeseries(batch=batch, network_output=network_output)\n",
    "            self.logger.experiment[figure_name].log(fig)\n",
    "            plt.close(fig)\n",
    "            \n",
    "        # Get NMAE per GSP, per forecast timestep, per day of year, and per hour of day\n",
    "        abs_error = (predicted_gsp_power - actual_gsp_power).abs().squeeze().cpu().detach()\n",
    "        nmae_per_example = abs_error.mean(dim=1)\n",
    "        metrics = {\n",
    "            \"loss\": neg_log_prob_loss,\n",
    "            \"NMAE\": nmae_loss.detach(),\n",
    "            \"NMAE_per_example\": nmae_per_example,\n",
    "            \"NMAE_per_forecast_timestep\": pd.Series(abs_error.mean(dim=0), index=np.arange(1, 5), name=batch_idx, dtype=np.float32),\n",
    "        }\n",
    "        for key in (\"gsp_id\", \"t0_hour_of_day\", \"t0_month\"):\n",
    "            metrics[f\"NMAE_per_{key}\"] = pd.Series(\n",
    "                nmae_per_example, \n",
    "                index=batch[key].cpu().numpy().astype(np.int32), \n",
    "                name=batch_idx,\n",
    "                dtype=np.float32\n",
    "            ).groupby(level=0).mean()\n",
    "            \n",
    "            \n",
    "        # 3. ************ \n",
    "        # save validation results\n",
    "        capacity = batch[\"capacity_mwp\"].cpu().numpy()\n",
    "        predictions = predicted_gsp_power.detach().cpu().numpy().squeeze()\n",
    "        truths = actual_gsp_power.detach().cpu().numpy().squeeze()\n",
    "        predictions = predictions * capacity\n",
    "        truths = truths * capacity\n",
    "\n",
    "        results = make_validation_results(\n",
    "            truths_mw=truths,\n",
    "            predictions_mw=predictions,\n",
    "            capacity_mwp=capacity,\n",
    "            gsp_ids=batch[\"gsp_id\"].cpu(),\n",
    "            batch_idx=batch_idx,\n",
    "            t0_datetimes_utc=pd.to_datetime(\n",
    "                batch[\"t0_datetime_UNIX_epoch\"].cpu().numpy().squeeze(), \n",
    "                unit=\"s\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # append so in 'validation_epoch_end' the file is saved\n",
    "        if batch_idx == 0:\n",
    "            self.results_dfs = []\n",
    "        self.results_dfs.append(results)\n",
    "        \n",
    "        # ***************\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def _training_or_validation_epoch_end(self, step_outputs: list[dict], tag: str) -> None:\n",
    "        # step_outputs is a list of dicts.  We want a dict of lists :)\n",
    "        metric_names = step_outputs[0].keys()\n",
    "        dict_of_lists_of_metrics: dict[str, list] = {metric_name: [] for metric_name in metric_names}\n",
    "        for step_output in step_outputs:\n",
    "            for metric_name, metric_value in step_output.items():\n",
    "                dict_of_lists_of_metrics[metric_name].append(metric_value)\n",
    "\n",
    "        # Loop through the metrics we're interested in\n",
    "        metrics_we_want = set(metric_names) - set([\"loss\", \"NMAE\", \"NMAE_per_example\"])\n",
    "        for metric_name in metrics_we_want:\n",
    "            metric_df = pd.concat(dict_of_lists_of_metrics[metric_name], axis=\"columns\")\n",
    "            mean_metric = metric_df.mean(axis=\"columns\")\n",
    "            if metric_name == \"NMAE_per_gsp_id\":\n",
    "                mean_metric = mean_metric.sort_values()\n",
    "            else: \n",
    "                mean_metric = mean_metric.sort_index()\n",
    "            # Plot!\n",
    "            fig, ax = plt.subplots(figsize=(40, 20))\n",
    "            mean_metric.plot.bar(ax=ax)\n",
    "            figure_name = f\"{tag}/plot/{metric_name}/epoch={self.current_epoch}\"\n",
    "            ax.set_ylabel(\"NMAE\")\n",
    "            ax.set_title(figure_name)\n",
    "            self.logger.experiment[figure_name].log(fig)\n",
    "            plt.close(fig)\n",
    "            \n",
    "        # Histogram of NMAE across all examples\n",
    "        nmae_per_example = np.concatenate(dict_of_lists_of_metrics[\"NMAE_per_example\"])\n",
    "        if not np.isnan(nmae_per_example).any():\n",
    "            fig, ax = plt.subplots(figsize=(20, 20))\n",
    "            ax.hist(nmae_per_example, bins=64)\n",
    "            ax.set_title(\"Histogram of NMAE across all examples\")\n",
    "            ax.set_ylabel(\"count\")\n",
    "            ax.set_xlabel(\"NMAE\")\n",
    "            self.logger.experiment[f\"{tag}/plot/hist_of_NMAE_per_example/epoch={self.current_epoch}\"].log(fig)\n",
    "            plt.close(fig)\n",
    "            \n",
    "        # 4. *********\n",
    "        \n",
    "        \n",
    "        save_validation_results_to_logger(results_dfs=self.results_dfs,\n",
    "                                          results_file_name=self.results_file_name + tag,\n",
    "                                          current_epoch=self.current_epoch,\n",
    "                                          logger=self.logger)\n",
    "        \n",
    "        # ********\n",
    "\n",
    "    def training_step(self, batch: dict[str, torch.Tensor], batch_idx: int):\n",
    "        return self._training_or_validation_step(batch=batch, batch_idx=batch_idx, tag=\"train\")\n",
    "    \n",
    "    def validation_step(self, batch: dict[str, torch.Tensor], batch_idx: int):\n",
    "        return self._training_or_validation_step(batch=batch, batch_idx=batch_idx, tag=\"validation\")\n",
    "    \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        self._training_or_validation_epoch_end(training_step_outputs, tag=\"train\")\n",
    "        \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        self._training_or_validation_epoch_end(validation_step_outputs, tag=\"validation\")\n",
    "        \n",
    "    # 4. *********\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        save_validation_results_to_logger(results_dfs=self.results_dfs,\n",
    "                                          results_file_name=self.results_file_name + \"test\",\n",
    "                                          current_epoch=self.current_epoch,\n",
    "                                          logger=self.logger)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.validation_step(batch=batch, batch_idx=batch_idx)\n",
    "        \n",
    "    # ********\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f39303e4-d885-48b4-ac62-77b2da663704",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "#model = Model.load_from_checkpoint(\"/home/jack/dev/ocf/predict_pv_yield/notebooks/.neptune/Untitled/PRED-759/checkpoints/epoch=5-step=25199.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89159408-4adc-4b30-9b74-6b1b7c7ae384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/OpenClimateFix/predict-pv-yield/e/PRED-762\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "neptune_logger = NeptuneLogger(\n",
    "    project=\"OpenClimateFix/predict-pv-yield\",\n",
    "    prefix=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e5e4289-0570-4b26-ab13-9ce4094cea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='NMAE/validation',\n",
    "    #filename='{epoch:02d}-MAE_MW_24hr_validation{val_loss:.2f}',\n",
    "    save_top_k=5,\n",
    "    mode='min',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee4637b7-8273-4fd7-a25d-d7cdfbecfd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=[1], logger=neptune_logger, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2c5fc-f4a5-456c-93c5-929da7947071",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c34adc-56c1-44b8-8f04-195c74dc5085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_pv_yield",
   "language": "python",
   "name": "predict_pv_yield"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
